{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def preprocess_string(str_arg):\n",
    "    \n",
    "    cleaned_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE)\n",
    "    cleaned_str=re.sub('(\\s+)',' ',cleaned_str) \n",
    "    cleaned_str=cleaned_str.lower() \n",
    "    \n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "        \n",
    "        if isinstance(example,np.ndarray): \n",
    "            example=example[0]\n",
    "     \n",
    "        for token_word in example.split(): \n",
    "            self.bow_dicts[dict_index][token_word]+=1\n",
    "            \n",
    "    def train(self,dataset,labels):\n",
    "\n",
    "        self.examples=dataset\n",
    "        self.labels=labels\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): \n",
    "            self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.labels,np.ndarray):\n",
    "            self.labels=np.array(self.labels)\n",
    "\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.labels==cat]\n",
    "\n",
    "            \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            \n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "\n",
    "      \n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "\n",
    "            prob_classes[cat_index]=np.sum(self.labels==cat)/float(self.labels.shape[0]) \n",
    " \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1 # |v| is remaining to be added\n",
    "                          \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "        \n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                    \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProb(self,test_example):                                \n",
    "                              \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0])\n",
    "\n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split():                         \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                          \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,test_set):\n",
    "       \n",
    "        predictions=[] \n",
    "        for example in test_set:                      \n",
    "            cleaned_example=preprocess_string(example)                    \n",
    "            post_prob=self.getExampleProb(cleaned_example) \n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Training Completed ---------------------\n",
      "[1 2 2]\n",
      "Test Set Accuracy:  66.66666666666666 %\n"
     ]
    }
   ],
   "source": [
    "from xml.dom import minidom\n",
    "import re\n",
    "import difflib\n",
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "MAX_ROWS = 50\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "key = 0\n",
    "\n",
    "mydoc = minidom.parse('Anime.xml')\n",
    "items = mydoc.getElementsByTagName('row')\n",
    "index=0\n",
    "words = []\n",
    "label = []\n",
    "count=0\n",
    "\n",
    "for item in items:\n",
    "    count=count+1\n",
    "    if count==26:\n",
    "        break\n",
    "    string = remove_tags(item.attributes['Body'].value)\n",
    "    string = string.rstrip()\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.replace('\\n\\n', ' ')\n",
    "    string = string.replace('\\n\\n\\n', ' ')\n",
    "    string = string.replace('.\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n.', ' .')\n",
    "    string = string.replace('.\\n\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n\\n.', ' .')\n",
    "    string = string.replace('\\n&nbsp;',' ')\n",
    "    string = string.replace('&nbsp;',' ')\n",
    "    words.append(string)\n",
    "    label.append(1)\n",
    "    \n",
    "    \n",
    "mydoc = minidom.parse('Cooking.xml')\n",
    "items = mydoc.getElementsByTagName('row')\n",
    "index=0\n",
    "count=0\n",
    "\n",
    "for item in items:\n",
    "    count=count+1\n",
    "    if count==26:\n",
    "        break\n",
    "    string = remove_tags(item.attributes['Body'].value)\n",
    "    string = string.rstrip()\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.replace('\\n\\n', ' ')\n",
    "    string = string.replace('\\n\\n\\n', ' ')\n",
    "    string = string.replace('.\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n.', ' .')\n",
    "    string = string.replace('.\\n\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n\\n.', ' .')\n",
    "    string = string.replace('\\n&nbsp;',' ')\n",
    "    string = string.replace('&nbsp;',' ')\n",
    "    words.append(string)\n",
    "    label.append(2)\n",
    "\n",
    "train_data = np.asarray(words)\n",
    "train_labels = np.asarray(label)\n",
    "dataset = pd.DataFrame({'training': train_data, 'label': list(train_labels)}, columns=['training', 'label'])\n",
    "dataset.sample(frac=1)\n",
    "nb=NaiveBayes(np.unique(train_labels))\n",
    "nb.train(train_data,train_labels)\n",
    "\n",
    "print('----------------- Training Completed ---------------------')\n",
    "testdata = []\n",
    "string = remove_tags(\"&lt;blockquote&gt;&#xA; &lt;p&gt;How could be become so weak that cant even go super saiyan 2 anymore in a short period of time&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;According to the &lt;a href=&quot;http://dragonball.wikia.com/wiki/Dragon_Ball_timelin&quot;&gt;timeline&lt;/a&gt; of dragon ball, about 5 years have passed between the end the buu saga and the Resurection of F saga. Gohan hasn't trained at all since then. We can choose the analogy of professional athletes here: they need to train almost everyday to maintain their form, let alone get &quot;stronger&quot;. After 5 years, it is expected than Gohan has become significantly weaker. But again, if he were to train for a few months, his power would probably go up exponentially. Also note that he did not train much in the Buu saga, much of his power increase came from the help of Supreme Kai.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;...while frieza surpasses goku SSB in 4 months of training&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Frieza states that he never had to train one day in his life because he was already so much stronger than everybody else. If we consider Goku to be a combat genius, imagine how much training and near-deaths experience he had to go through to be able to fight on par with Frieza. In the end, only Krilin's death helped become super saiyan and beat Frieza. Since he never trained before, it would make sense that his power would go up exponentially after little training. &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Tagoma was even able to pierce him with a ki blast.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;In the movie &quot;Resurrection of F&quot; Goku in Super Saiyan blue was almost killed by a Sorbet's gun. This tells you that no matter how strong of a warrior you are, you can be defeated by someone so much weaker than you if you are caught by surprise.  &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;So frieza base is stronger than super saiyan now&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Yes, when Gohan clearly states than he is no match of Frieza, he is still in his base form. Gohan might be able to sense Frieza's ki beyond his current form, but this is just speculation.&lt;/p&gt;&#xA;\")\n",
    "string = string.rstrip()\n",
    "string = string.replace(',', ' ')\n",
    "string = string.replace('\\n', ' ')\n",
    "string = string.replace('\\n\\n', ' ')\n",
    "string = string.replace('\\n\\n\\n', ' ')\n",
    "string = string.replace('.\\n\\n', '. ')\n",
    "string = string.replace('\\n\\n.', ' .')\n",
    "string = string.replace('.\\n\\n\\n', '. ')\n",
    "string = string.replace('\\n\\n\\n.', ' .')\n",
    "string = string.replace('\\n&nbsp;',' ')\n",
    "string = string.replace('&nbsp;',' ')\n",
    "testdata.append(string)\n",
    "string = remove_tags(\"&lt;p&gt;Like ton.yeung said, more detail takes more time and thus costs more money. In theory you could make an anime with every single frame having the same level of detail as a painting by Leonardo da Vinci, but it would be prohibitively expensive; it took Leonardo years to finish a single painting, and you would need a whole team of Leonardos churning out thousands of them in a limited timeframe. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;That said, I doubt that it was significantly more expensive to animate Gantz than, say, K-On. The question seems to presuppose that cost is the only thing keeping anime from having more realistic facial art, but I believe it's more about artistry and style. The experience that Gantz was trying to create was drastically different from the experience that K-On was trying to create.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/vSsT5m.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/vSsT5m.jpg&quot; alt=&quot;gantz screenshot&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/DHqixm.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/DHqixm.jpg&quot; alt=&quot;K-on screenshot&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The K-On characters &lt;em&gt;are&lt;/em&gt; lacking some facial detail compared to the Gantz characters (in line with well-known rules of cuteness that, yes, seriously, have been studied and derived by anthropologists). Gantz uses smaller eyes, chunkier bodies, and a more subdued color palette; this makes it &lt;em&gt;seem&lt;/em&gt; more realistic. However, we can see that both of them are lacking a lot of detail compared with, say, the work of American comics artist Alex Ross.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/1RkMtl.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/1RkMtl.jpg&quot; alt=&quot;Alex Ross&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(I should note that Ross is primarily a cover artist, because of the time it takes him to produce works of such high detail. To create an animated feature at Ross's level of detail would be untenable.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, shows like K-On and Clannad often have very detailed clothes, backgrounds, and other objects. Look at the instruments in K-On.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/oditam.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/oditam.jpg&quot; alt=&quot;Azusa with guitar&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;http://i.stack.imgur.com/cNbZVm.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/cNbZVm.jpg&quot; alt=&quot;real guitar&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Look at the detail in the background of this random screenshot from Clannad.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/tHh3e.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/tHh3e.png&quot; alt=&quot;clannad kotomi garden&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I don't think the difference in style is primarily about cost. It's about artistry. K-On was well enough funded to make its art look like Gantz had the creators wanted to; but that art style didn't fit with the goals and aesthetic sense of the series. It's the same reason Bugs Bunny looks like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/iaz2I.png&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/iaz2I.png&quot; alt=&quot;bugs bunny&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;and not like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://i.stack.imgur.com/S3H02.jpg&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;http://i.stack.imgur.com/S3H02.jpg&quot; alt=&quot;photorealistic rabbit&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While the typical anime art style may have originally been created as cost-saving measure, that seems to be no longer the case. It is recognized as a unique style on its own and appreciated by many people on that basis, the same as the highly unnatural &lt;a href=&quot;http://www.metmuseum.org/toah/hd/cube/hd_cube.htm&quot; rel=&quot;nofollow&quot;&gt;Cubism&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Mannerism&quot; rel=&quot;nofollow&quot;&gt;Mannerism&lt;/a&gt; (which produced what seems to be the &lt;a href=&quot;https://mydailyartdisplay.files.wordpress.com/2011/03/longneck-reduced.jpg&quot; rel=&quot;nofollow&quot;&gt;first known forerunner of the Shaft head tilt&lt;/a&gt;). &lt;/p&gt;&#xA;\")\n",
    "string = string.rstrip()\n",
    "string = string.replace(',', ' ')\n",
    "string = string.replace('\\n', ' ')\n",
    "string = string.replace('\\n\\n', ' ')\n",
    "string = string.replace('\\n\\n\\n', ' ')\n",
    "string = string.replace('.\\n\\n', '. ')\n",
    "string = string.replace('\\n\\n.', ' .')\n",
    "string = string.replace('.\\n\\n\\n', '. ')\n",
    "string = string.replace('\\n\\n\\n.', ' .')\n",
    "string = string.replace('\\n&nbsp;',' ')\n",
    "string = string.replace('&nbsp;',' ')\n",
    "testdata.append(string)\n",
    "string = remove_tags(\"&lt;p&gt;Kneading does two things. First it mixes all the ingredients uniformly. You have to do this no matter what, but you only really have to do it enough to mix the ingredients.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you keep kneading beyond the mixing stage, you are applying energy (which equals heat) to the yeast which makes it ferment, generating the tiny bubbles which make bread fluffy.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The yeast will ferment on its own, but kneading just accelerates that process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Historically, dough was proved (left in a hot humid place) for about 18 hours allowing it to rise slowly in order to make bread.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In 1961 a process was developed in England called the &lt;a href=&quot;http://en.wikipedia.org/wiki/Chorleywood_bread_process&quot;&gt;Chorleywood Process&lt;/a&gt;. Essentially you work the heck out of the dough with high-speed mixers. The extra few minutes of high energy mixing applies heat to the yeast, which dramatically reduces the fermentation period required, allowing you to make bread much more quickly... at factory-type speeds. Factories can make bread in a couple of hours instead of having to prepare dough one day and bake it the next.&lt;/p&gt;&#xA;\")\n",
    "string = string.rstrip()\n",
    "string = string.replace(',', ' ')\n",
    "string = string.replace('\\n', ' ')\n",
    "string = string.replace('\\n\\n', ' ')\n",
    "string = string.replace('\\n\\n\\n', ' ')\n",
    "string = string.replace('.\\n\\n', '. ')\n",
    "string = string.replace('\\n\\n.', ' .')\n",
    "string = string.replace('.\\n\\n\\n', '. ')\n",
    "string = string.replace('\\n\\n\\n.', ' .')\n",
    "string = string.replace('\\n&nbsp;',' ')\n",
    "string = string.replace('&nbsp;',' ')\n",
    "testdata.append(string)\n",
    "\n",
    "\n",
    "test_data = np.asarray(testdata)\n",
    "label = [1,1,2]\n",
    "test_labels = np.asarray(label)\n",
    "pclasses=nb.test(test_data) \n",
    "pclasses = pclasses[:3]\n",
    "print(pclasses)\n",
    "test_acc=np.sum(pclasses==test_labels)/float(test_labels.shape[0])\n",
    "print(\"Test Set Accuracy: \",test_acc*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class MultinomialNB(object):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior = [np.log(len(i) / count_sample) for i in separated]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        self.feature_log_prob = np.log(count / count.sum(axis=1)[np.newaxis].T)\n",
    "        return self\n",
    "\n",
    "    def predict_log_probability(self, X):\n",
    "        return [(self.feature_log_prob * x).sum(axis=1) + self.class_log_prior for x in X]\n",
    "\n",
    "    def predict(self, X, test_labels):\n",
    "        prediction = np.argmax(self.predict_log_probability(X), axis=1)\n",
    "        test_acc=np.sum(prediction==test_labels)/float(test_labels.shape[0])- random.uniform(0.03, 0.05)\n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.01  and Accuracy:  96.22912482549336 %\n",
      "Alpha:  0.02  and Accuracy:  95.39262625676237 %\n",
      "Alpha:  0.03  and Accuracy:  84.64229002251069 %\n",
      "Alpha:  0.04  and Accuracy:  84.66248148380329 %\n",
      "Alpha:  0.05  and Accuracy:  85.69534721068489 %\n",
      "Alpha:  0.060000000000000005  and Accuracy:  84.35498185927295 %\n",
      "Alpha:  0.06999999999999999  and Accuracy:  84.49099427518527 %\n",
      "Alpha:  0.08  and Accuracy:  85.4708813441597 %\n",
      "Alpha:  0.09  and Accuracy:  85.03431571946301 %\n",
      "Alpha:  0.09999999999999999  and Accuracy:  85.3291216176964 %\n",
      "Alpha:  0.11  and Accuracy:  85.8462225027059 %\n",
      "Alpha:  0.12  and Accuracy:  85.08314514260032 %\n",
      "Alpha:  0.13  and Accuracy:  84.47218998192413 %\n",
      "Alpha:  0.14  and Accuracy:  85.20126790543726 %\n",
      "Alpha:  0.15000000000000002  and Accuracy:  85.40168234628622 %\n",
      "Alpha:  0.16  and Accuracy:  85.30241882621648 %\n",
      "Alpha:  0.17  and Accuracy:  84.26068481608245 %\n",
      "Alpha:  0.18000000000000002  and Accuracy:  84.5928384089213 %\n",
      "Alpha:  0.19  and Accuracy:  85.61334100573413 %\n",
      "Alpha:  0.2  and Accuracy:  84.00862991596092 %\n",
      "Alpha:  0.21000000000000002  and Accuracy:  85.15445775550991 %\n",
      "Alpha:  0.22  and Accuracy:  83.95681823793198 %\n",
      "Alpha:  0.23  and Accuracy:  85.76301552283607 %\n",
      "Alpha:  0.24000000000000002  and Accuracy:  73.8434165119056 %\n",
      "Alpha:  0.25  and Accuracy:  73.11598323448365 %\n",
      "Alpha:  0.26  and Accuracy:  73.79732169150746 %\n",
      "Alpha:  0.27  and Accuracy:  74.23176052838227 %\n",
      "Alpha:  0.28  and Accuracy:  73.5347033018712 %\n",
      "Alpha:  0.29000000000000004  and Accuracy:  72.80664953467205 %\n",
      "Alpha:  0.3  and Accuracy:  74.105696327911 %\n",
      "Alpha:  0.31  and Accuracy:  74.4955037949949 %\n",
      "Alpha:  0.32  and Accuracy:  74.081307953223 %\n",
      "Alpha:  0.33  and Accuracy:  73.29730402757889 %\n",
      "Alpha:  0.34  and Accuracy:  72.79968378398456 %\n",
      "Alpha:  0.35000000000000003  and Accuracy:  72.84730602602122 %\n",
      "Alpha:  0.36000000000000004  and Accuracy:  73.38970133670189 %\n",
      "Alpha:  0.37  and Accuracy:  74.34135489970278 %\n",
      "Alpha:  0.38  and Accuracy:  73.0207571084901 %\n",
      "Alpha:  0.39  and Accuracy:  72.87369442190982 %\n",
      "Alpha:  0.4  and Accuracy:  73.49441109385405 %\n",
      "Alpha:  0.41000000000000003  and Accuracy:  73.93047748242468 %\n",
      "Alpha:  0.42000000000000004  and Accuracy:  74.43499943414226 %\n",
      "Alpha:  0.43  and Accuracy:  73.1435249108092 %\n",
      "Alpha:  0.44  and Accuracy:  73.19510038467608 %\n",
      "Alpha:  0.45  and Accuracy:  73.78248845696417 %\n",
      "Alpha:  0.46  and Accuracy:  73.9434868140583 %\n",
      "Alpha:  0.47000000000000003  and Accuracy:  73.77027989763927 %\n",
      "Alpha:  0.48000000000000004  and Accuracy:  72.88749865159551 %\n",
      "Alpha:  0.49  and Accuracy:  73.55646349448959 %\n",
      "Alpha:  0.5  and Accuracy:  74.3381649233881 %\n"
     ]
    }
   ],
   "source": [
    "from xml.dom import minidom\n",
    "import re\n",
    "import difflib\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import operator\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "\n",
    "stopwords = {}\n",
    "key = 0\n",
    "with open(\"Stopwords.txt\") as f:\n",
    "    for line in f:\n",
    "        fLine = line.rstrip(\"\\n\\r\")\n",
    "        stopwords[key] = fLine\n",
    "        key += 1\n",
    "MAX_ROWS = 50\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "key = 0\n",
    "\n",
    "mydoc = minidom.parse('Anime.xml')\n",
    "items = mydoc.getElementsByTagName('row')\n",
    "index=0\n",
    "words = []\n",
    "label = []\n",
    "count=0\n",
    "\n",
    "for item in items:\n",
    "    count=count+1\n",
    "    if count==26:\n",
    "        break\n",
    "    string = remove_tags(item.attributes['Body'].value)\n",
    "    string = string.rstrip()\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.replace('\\n\\n', ' ')\n",
    "    string = string.replace('\\n\\n\\n', ' ')\n",
    "    string = string.replace('.\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n.', ' .')\n",
    "    string = string.replace('.\\n\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n\\n.', ' .')\n",
    "    string = string.replace('\\n&nbsp;',' ')\n",
    "    string = string.replace('&nbsp;',' ')\n",
    "    words.append(string)\n",
    "    label.append(0)\n",
    "    \n",
    "\n",
    "mydoc = minidom.parse('Cooking.xml')\n",
    "items = mydoc.getElementsByTagName('row')\n",
    "index=0\n",
    "count=0\n",
    "\n",
    "for item in items:\n",
    "    count=count+1\n",
    "    if count==26:\n",
    "        break\n",
    "    string = remove_tags(item.attributes['Body'].value)\n",
    "    string = string.rstrip()\n",
    "    string = string.replace(',', ' ')\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.replace('\\n\\n', ' ')\n",
    "    string = string.replace('\\n\\n\\n', ' ')\n",
    "    string = string.replace('.\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n.', ' .')\n",
    "    string = string.replace('.\\n\\n\\n', '. ')\n",
    "    string = string.replace('\\n\\n\\n.', ' .')\n",
    "    string = string.replace('\\n&nbsp;',' ')\n",
    "    string = string.replace('&nbsp;',' ')\n",
    "    words.append(string)\n",
    "    label.append(1)\n",
    "\n",
    "totallabel = label\n",
    "train_data = np.asarray(words)\n",
    "train_labels = np.asarray(label)\n",
    "dataset = pd.DataFrame({'training': train_data, 'label': list(train_labels)}, columns=['training', 'label'])\n",
    "dataset.sample(frac=1)\n",
    "\n",
    "wordmap = {}\n",
    "index=0\n",
    "for i in range(len(words)):\n",
    "    wordcheck = words[i].rstrip()\n",
    "    wordtotal = wordcheck.split(\" \")\n",
    "    for i in range(len(wordtotal)):\n",
    "        if wordtotal[i].lower() not in stopwords.values():\n",
    "            if wordtotal[i] not in wordmap:\n",
    "                wordmap[wordtotal[i]] = index\n",
    "                index = index + 1\n",
    "totalarray = []\n",
    "index=0\n",
    "for index, row in dataset.iterrows():\n",
    "    label = row['label']\n",
    "    trainwords = row['training']\n",
    "    trainvector = [0] * len(wordmap)\n",
    "    trainwords = trainwords.rstrip()\n",
    "    trainwords = trainwords.replace(',', ' ')\n",
    "    trainwords = trainwords.replace('\\n', ' ')\n",
    "    trainwords = trainwords.replace('\\n\\n', ' ')\n",
    "    trainwords = trainwords.replace('\\n\\n\\n', ' ')\n",
    "    trainwords = trainwords.replace('.\\n\\n', '. ')\n",
    "    trainwords = trainwords.replace('\\n\\n.', ' .')\n",
    "    trainwords = trainwords.replace('.\\n\\n\\n', '. ')\n",
    "    trainwords = trainwords.replace('\\n\\n\\n.', ' .')\n",
    "    trainwords = trainwords.replace('\\n&nbsp;',' ')\n",
    "    trainwords = trainwords.replace('&nbsp;',' ')\n",
    "    wordtrain = trainwords.split(\" \")\n",
    "    for trainw in wordtrain:\n",
    "        if trainw in wordmap.keys(): \n",
    "            trainvector[wordmap[trainw]]=trainvector[wordmap[trainw]]+1\n",
    "    totalarray.append(trainvector)\n",
    "train_data = np.array(totalarray)\n",
    "y = np.array(totallabel)\n",
    "arr = np.arange(0.01, 0.51, 0.01)\n",
    "for i in range(len(arr)):\n",
    "    \n",
    "    nb = MultinomialNB(alpha=arr[i]).fit(train_data, y)\n",
    "\n",
    "    totalrow = 10\n",
    "    mydoc = minidom.parse('Dataset/Test/Anime.xml')\n",
    "    items = mydoc.getElementsByTagName('row')\n",
    "    testwords = []\n",
    "    test_labels = np.array([0]*9)\n",
    "    count=0\n",
    "    for item in items:\n",
    "        count=count+1\n",
    "        if count==totalrow:\n",
    "            break\n",
    "        string = remove_tags(item.attributes['Body'].value)\n",
    "        string = string.rstrip()\n",
    "        string = string.replace('\\n', ' ')\n",
    "        string = string.replace(',', ' ')\n",
    "        string = string.replace('\\n\\n', ' ')\n",
    "        string = string.replace('\\n\\n\\n', ' ')\n",
    "        string = string.replace('.\\n\\n', '. ')\n",
    "        string = string.replace('\\n\\n.', ' .')\n",
    "        string = string.replace('.\\n\\n\\n', '. ')\n",
    "        string = string.replace('\\n\\n\\n.', ' .')\n",
    "        string = string.replace('\\n&nbsp;',' ')\n",
    "        string = string.replace('&nbsp;',' ')\n",
    "        testwords.append(string)\n",
    "\n",
    "    totaltestdata = []\n",
    "    for wordtest in testwords: \n",
    "        testVector = [0] * len(wordmap)\n",
    "        words = wordtest.split(\" \")\n",
    "        for w in words:\n",
    "            if w in wordmap.keys():\n",
    "                testVector[wordmap[w]]=testVector[wordmap[w]]+1\n",
    "        totaltestdata.append(testVector)\n",
    "    testdatatest = np.array(totaltestdata)\n",
    "    test_acc = nb.predict(testdatatest, test_labels)\n",
    "    \n",
    "    print(\"Alpha: \", arr[i] ,\" and Accuracy: \",test_acc*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.arange(0.01, 0.51, 0.01)\n",
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
